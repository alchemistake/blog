---
layout: post
title: It turns out it is hard to remember 2 month old code
date: 2022-02-04 22:21 +0100
categories: animation-transformer
tags: machine-learning code
---
So I have a theory that transformers can generate 3D Animations since language and animation are alike in many aspects. The main difference is Transformers are designed with classification and distinct tokens and 3D animations work with continuous numbers. So It requires some conversion even before I can experiment and see whether it will work or not. I started working on this around last December on and off. I started with re-learning the basics and reading some papers on the topic. As the new year starts I decided to focus on it actively rather than beating around the bush.

I thought I must not be the only guy thinking of the same thing and I found there are some examples of conversion to work with time-series, and technically 3D animations are time series of composite angles. I found some examples online. Nothing was documented clearly but it can be a good point to start, if I can cut the corners I will since I only have 3 hours every day, more or less. I started with experimenting with [the project that has the best documentation.](https://github.com/maxjcohen/transformer)

In the middle of the day, I realized they stripped out the parts I want to keep and added their own implementation. Then, I just realized I figured the same thing 2 months ago, too. Due to their implementation differences, my data was not fitting at all. After deciding, off-the-shelf implementations are not fitting me, I decided to do it myself. I revised my notes on the papers. Rather than re-implementing the transformer part of the code I used [annoted version of "Attention is All You Need" paper](http://nlp.seas.harvard.edu/2018/04/03/attention.html) as a starting ground. Of course, it came with its own problems.

My computer didn't have a torch or anything installed because [last time I was trying to train anything,]({% post_url 2021-12-11-what-i-learned-this-week-2021-49 %}#tuesday) I tried to use WSL since I'm more familiar with Linux, it turns out it was a mistake. Long story short: if you install a special driver that gives you CUDA powers on WSL and if you have a G-SYNC enabled monitor connected some glitches cause horrible stuff like frame drops and intermittent green flashes. Even if WSL is not actively running, the problems continue. Rather than having system-wide problems, I decided to switch to windows' python. Thus I needed to re-download and install everything from driver to torch package. It used an outdated version of PyTorch (which might not be outdated at the time of release) and it was not available for Windows, so I spent some time porting the code to the latest PyTorch version. Thankfully, it was straightforward and fast.

So that wasted almost all the time I have for this week, I decided to adapt an old Facebook adage: “Move fast and break things.” I decided to convert just a tiny part of the code and try to run it. Of course, everything starts to give an error. So I fix those until everything is converted to work with numbers. I'm not done with that conversion cycle, yet. When it is ready we will see whether the theory will work or not.